[
  {
    "name": "BERT",
    "image": "https://d827xgdhgqbnd.cloudfront.net/wp-content/uploads/2019/04/09110726/Bert-Head.png",
    "github": "https://github.com/google-research/bert",
    "description": "BERT steht für \"Bidirectional Encoder Representations from Transformers\" und wurde 2018 von Jacob Devlin et al. bei Google entwickelt und mit Hilfe der Texte von Wikipedia (2,5 Milliarden Wörter) und dem BookCorpus(800 Millionen Wörter) trainiert. BERT nutzt eine weiterentwickelte Form der Transformer-Architektur und erstellt Embeddings indem er mit zwei Unsupervised Learning-Aufgaben trainiert.",
    "advantages": "",
    "disadvantages": "",
    "download": "https://huggingface.co/bert-base-uncased",
    "paper": "https://arxiv.org/abs/1810.04805",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "DistilBERT",
    "image": "https://huggingface.co/favicon.ico",
    "github": "https://github.com/huggingface/transformers/tree/master/examples/distillation",
    "description": "DistilBERT wurde von huggingface.co entwickelt um dem stetigen Wachstum der NLP-Modelle entgegen zu wirken. Basierend auf dem BERT-Modell, ist DistilBERT ein reduziertes Modell, welches traniert wurde, um das Verhalten des großen Modells (in diesem Fall BERT) zu reproduzieren.",
    "advantages": "",
    "disadvantages": "",
    "download": "https://huggingface.co/distilbert-base-uncased",
    "paper": "https://arxiv.org/abs/1910.01108",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "RoBERTa",
    "image": "https://d827xgdhgqbnd.cloudfront.net/wp-content/uploads/2019/04/09110726/Bert-Head.png",
    "github": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
    "description": "RoBERTa basiert auf dem BERT-Modell, welches mit Hilfe einer Maskierungsstrategie erlernt, versteckte (maskierte) Teile eines Textes zu rekonstruieren. RoBERTa nutzt diese Maskierung, wird im Vergleich zu BERT jedoch mit deutlich kleineren Batches, mehr Daten und für einen längeren Zeitraum trainiert. Dies erlaubt eine größere Genauigkeit in ML-Aufgaben.",
    "advantages": "",
    "disadvantages": "",
    "download": "https://huggingface.co/roberta-base",
    "paper": "https://arxiv.org/abs/1907.11692",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "ALBERT",
    "image": "https://d827xgdhgqbnd.cloudfront.net/wp-content/uploads/2019/04/09110726/Bert-Head.png",
    "github": "https://github.com/google-research/albert",
    "description": "ALBERT steht für \"A Lite BERT\" und versucht, die Größe des BERT-Modells zu verringern, ohne an Genauigkeit zu verlieren. Dies wird durch eine Paramterübergabe zwischen den Schichten des Transformer-basierten neuronalen Netzes erlangt.",
    "advantages": "",
    "disadvantages": "",
    "download": "https://huggingface.co/albert-base-v2",
    "paper": "https://arxiv.org/abs/1909.11942",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "XLNet",
    "image": "",
    "github": "https://github.com/zihangdai/xlnet",
    "description": "",
    "advantages": "",
    "disadvantages": "",
    "download": "https://huggingface.co/xlnet-base-cased",
    "paper": "https://arxiv.org/abs/1906.08237",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "T5",
    "image": "",
    "github": "https://github.com/google-research/text-to-text-transfer-transformer",
    "description": "",
    "advantages": "",
    "disadvantages": "",
    "download": "https://huggingface.co/t5-base",
    "paper": "https://arxiv.org/abs/1910.10683",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "GPT-2",
    "image": "",
    "github": "https://github.com/openai/gpt-2",
    "description": "",
    "advantages": "",
    "disadvantages": "",
    "download": "https://huggingface.co/gpt2",
    "paper": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "fastText",
    "image": "https://miro.medium.com/max/1200/1*xRvz53WKr4koIKgmJZoRFQ.png",
    "github": "https://github.com/facebookresearch/fastText",
    "description": "",
    "advantages": "",
    "disadvantages": "",
    "download": "https://fasttext.cc/docs/en/english-vectors.html",
    "paper": "https://arxiv.org/abs/1607.04606",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "Word2Vec",
    "image": "",
    "github": "https://github.com/tmikolov/word2vec",
    "description": "",
    "advantages": "",
    "disadvantages": "",
    "download": "",
    "paper": "https://arxiv.org/abs/1301.3781",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "GloVe",
    "image": "",
    "github": "https://github.com/stanfordnlp/GloVe",
    "description": "",
    "advantages": "",
    "disadvantages": "",
    "download": "",
    "paper": "https://nlp.stanford.edu/pubs/glove.pdf",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  },
  {
    "name": "Bag-of-Words",
    "image": "",
    "github": "https://github.com/rahulvasaikar/Bag-of-words",
    "description": "",
    "advantages": "",
    "disadvantages": "",
    "download": "",
    "paper": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwi42_KYnMPsAhVB3qQKHUnNCAsQFjACegQIAhAC&url=https%3A%2F%2Fcs.nju.edu.cn%2Fzhouzh%2Fzhouzh.files%2Fpublication%2Fijmlc10.pdf&usg=AOvVaw3o9tGEWV4In1hiwFzIdsQj",
    "specs": {
      "language": 2,
      "length": 10000,
      "data": 100000,
      "mistakes": "true",
      "context": "true"
    }
  }
]