[
  {
    "name": "BERT",
    "github": "https://github.com/google-research/bert",
    "description": "BERT steht für \"Bidirectional Encoder Representations from Transformers\" und wurde 2018 von Jacob Devlin et al. bei Google entwickelt und mit Hilfe der Texte von Wikipedia (2,5 Milliarden Wörter) und dem BookCorpus(800 Millionen Wörter) trainiert. BERT nutzt eine weiterentwickelte Form der Transformer-Architektur und erstellt Embeddings indem er mit zwei Unsupervised Learning-Aufgaben trainiert.",
    "paper": "https://arxiv.org/abs/1810.04805",
    "languages": [
      {"multi": ""}
    ],
    "textLength": 1000,
    "training": "untrained",
    "gpu": 25,
    "glue": 82.1
  },
  {
    "name": "BERT-base",
    "github": "https://github.com/google-research/bert",
    "description": "BERT steht für \"Bidirectional Encoder Representations from Transformers\" und wurde 2018 von Jacob Devlin et al. bei Google entwickelt und mit Hilfe der Texte von Wikipedia (2,5 Milliarden Wörter) und dem BookCorpus(800 Millionen Wörter) trainiert. BERT nutzt eine weiterentwickelte Form der Transformer-Architektur und erstellt Embeddings indem er mit zwei Unsupervised Learning-Aufgaben trainiert.",
    "paper": "https://arxiv.org/abs/1810.04805",
    "languages": [
      {"multi": "https://huggingface.co/bert-base-multilingual-cased"},
      {"english": "https://huggingface.co/bert-base-uncased"},
      {"german": "https://huggingface.co/bert-base-german-cased"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 8,
    "glue": 79.6
  },
  {
    "name": "BERT-large",
    "github": "https://github.com/google-research/bert",
    "description": "BERT steht für \"Bidirectional Encoder Representations from Transformers\" und wurde 2018 von Jacob Devlin et al. bei Google entwickelt und mit Hilfe der Texte von Wikipedia (2,5 Milliarden Wörter) und dem BookCorpus(800 Millionen Wörter) trainiert. BERT nutzt eine weiterentwickelte Form der Transformer-Architektur und erstellt Embeddings indem er mit zwei Unsupervised Learning-Aufgaben trainiert.",
    "paper": "https://arxiv.org/abs/1810.04805",
    "languages": [
      {"english": "https://huggingface.co/bert-large-cased"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 24,
    "glue": 82.1
  },
  {
    "name": "DistilBERT-base",
    "github": "https://github.com/huggingface/transformers/tree/master/examples/distillation",
    "description": "DistilBERT wurde von huggingface.co entwickelt um dem stetigen Wachstum der NLP-Modelle entgegen zu wirken. Basierend auf dem BERT-Modell, ist DistilBERT ein reduziertes Modell, welches traniert wurde, um das Verhalten des großen Modells (in diesem Fall BERT) zu reproduzieren.",
    "paper": "https://arxiv.org/abs/1910.01108",
    "languages": [
      {"multi": "https://huggingface.co/distilbert-base-multilingual-cased"},
      {"english": "https://huggingface.co/distilbert-base-cased"},
      {"german": "https://huggingface.co/distilbert-base-german-cased"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 8,
    "glue": 77.0
  },
  {
    "name": "RoBERTa",
    "github": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
    "description": "RoBERTa basiert auf dem BERT-Modell, welches mit Hilfe einer Maskierungsstrategie erlernt, versteckte (maskierte) Teile eines Textes zu rekonstruieren. RoBERTa nutzt diese Maskierung, wird im Vergleich zu BERT jedoch mit deutlich kleineren Batches, mehr Daten und für einen längeren Zeitraum trainiert. Dies erlaubt eine größere Genauigkeit in ML-Aufgaben.",
    "paper": "https://arxiv.org/abs/1907.11692",
    "languages": [
      {"multi": ""}
    ],
    "textLength": 1000,
    "training": "untrained",
    "gpu": 25,
    "glue": 88.5
  },
  {
    "name": "RoBERTa-base",
    "github": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
    "description": "RoBERTa basiert auf dem BERT-Modell, welches mit Hilfe einer Maskierungsstrategie erlernt, versteckte (maskierte) Teile eines Textes zu rekonstruieren. RoBERTa nutzt diese Maskierung, wird im Vergleich zu BERT jedoch mit deutlich kleineren Batches, mehr Daten und für einen längeren Zeitraum trainiert. Dies erlaubt eine größere Genauigkeit in ML-Aufgaben.",
    "paper": "https://arxiv.org/abs/1907.11692",
    "languages": [
      {"english": "https://huggingface.co/roberta-base"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 8,
    "glue": 80.5
  },
  {
    "name": "RoBERTa-large",
    "github": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
    "description": "RoBERTa basiert auf dem BERT-Modell, welches mit Hilfe einer Maskierungsstrategie erlernt, versteckte (maskierte) Teile eines Textes zu rekonstruieren. RoBERTa nutzt diese Maskierung, wird im Vergleich zu BERT jedoch mit deutlich kleineren Batches, mehr Daten und für einen längeren Zeitraum trainiert. Dies erlaubt eine größere Genauigkeit in ML-Aufgaben.",
    "paper": "https://arxiv.org/abs/1907.11692",
    "languages": [
      {"english": "https://huggingface.co/roberta-base"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 24,
    "glue": 88.5
  },
  {
    "name": "ALBERT-base",
    "github": "https://github.com/google-research/albert",
    "description": "ALBERT steht für \"A Lite BERT\" und versucht, die Größe des BERT-Modells zu verringern, ohne an Genauigkeit zu verlieren. Dies wird durch eine Paramterübergabe zwischen den Schichten des Transformer-basierten neuronalen Netzes erlangt.",
    "paper": "https://arxiv.org/abs/1909.11942",
    "languages": [
      {"english": "https://huggingface.co/albert-base-v2"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 8,
    "glue": 89.4
  },
  {
    "name": "ALBERT-large",
    "github": "https://github.com/google-research/albert",
    "description": "ALBERT steht für \"A Lite BERT\" und versucht, die Größe des BERT-Modells zu verringern, ohne an Genauigkeit zu verlieren. Dies wird durch eine Paramterübergabe zwischen den Schichten des Transformer-basierten neuronalen Netzes erlangt.",
    "paper": "https://arxiv.org/abs/1909.11942",
    "languages": [
      {"english": "https://huggingface.co/albert-large-v2"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 24,
    "glue": 89.4
  },
  {
    "name": "XLNet",
    "github": "https://github.com/zihangdai/xlnet",
    "description": "XLNet wurde 2019 von Zhilin Yang et.al. entwickelt und basiert auf einem \"permutation language modeling\"-Prozess. Durch das Vertauschen der Wörter führt dazu, dass XLNet Abhängigkeiten in verschiedenen Kombinationen erlernt.",
    "paper": "https://arxiv.org/abs/1906.08237",
    "languages": [
      {"multi": ""}
    ],
    "textLength": 1000,
    "training": "untrained",
    "gpu": 25,
    "glue": 90.5
  },
  {
    "name": "XLNet-base",
    "github": "https://github.com/zihangdai/xlnet",
    "description": "XLNet wurde 2019 von Zhilin Yang et.al. entwickelt und basiert auf einem \"permutation language modeling\"-Prozess. Durch das Vertauschen der Wörter führt dazu, dass XLNet Abhängigkeiten in verschiedenen Kombinationen erlernt.",
    "paper": "https://arxiv.org/abs/1906.08237",
    "languages": [
      {"english": "https://huggingface.co/xlnet-base-cased"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 8,
    "glue": 85
  },
  {
    "name": "XLNet-large",
    "github": "https://github.com/zihangdai/xlnet",
    "description": "XLNet wurde 2019 von Zhilin Yang et.al. entwickelt und basiert auf einem \"permutation language modeling\"-Prozess. Durch das Vertauschen der Wörter führt dazu, dass XLNet Abhängigkeiten in verschiedenen Kombinationen erlernt.",
    "paper": "https://arxiv.org/abs/1906.08237",
    "languages": [
      {"english": "https://huggingface.co/xlnet-large-cased"}
    ],
    "textLength": 1000,
    "training": "trained",
    "gpu": 24
  ,
    "glue": 90.5
  },
  {
    "name": "T5",
    "github": "https://github.com/google-research/text-to-text-transfer-transformer",
    "description": "Text-To-Text Transfer Transformer (T5) wurde innerhalb von \"Google Reasearch\" von Colin Raffel et. al. entwickelt. Zugrunde liegt eine empirische Studie über die Effektivität verschiedener \"transfer learning\" Techniken und ein eigens entwickelter Trainings-Datensatz (C4). Sowohl Ein- als auch Ausgabe sind Texte.",
    "paper": "https://arxiv.org/abs/1910.10683",
    "languages": [
      {"multi": ""}
    ],
    "textLength": 1001,
    "training": "untrained",
    "gpu": 25,
    "glue": 90.3
  },
  {
    "name": "T5-base",
    "github": "https://github.com/google-research/text-to-text-transfer-transformer",
    "description": "Text-To-Text Transfer Transformer (T5) wurde innerhalb von \"Google Reasearch\" von Colin Raffel et. al. entwickelt. Zugrunde liegt eine empirische Studie über die Effektivität verschiedener \"transfer learning\" Techniken und ein eigens entwickelter Trainings-Datensatz (C4). Sowohl Ein- als auch Ausgabe sind Texte.",
    "paper": "https://arxiv.org/abs/1910.10683",
    "languages": [
      {"english": "https://huggingface.co/t5-base"}
    ],
    "textLength": 1001,
    "training": "trained",
    "gpu": 8,
    "glue": 82.7
  },
  {
    "name": "T5-large",
    "github": "https://github.com/google-research/text-to-text-transfer-transformer",
    "description": "Text-To-Text Transfer Transformer (T5) wurde innerhalb von \"Google Reasearch\" von Colin Raffel et. al. entwickelt. Zugrunde liegt eine empirische Studie über die Effektivität verschiedener \"transfer learning\" Techniken und ein eigens entwickelter Trainings-Datensatz (C4). Sowohl Ein- als auch Ausgabe sind Texte.",
    "paper": "https://arxiv.org/abs/1910.10683",
    "languages": [
      {"english": "https://huggingface.co/t5-large"}
    ],
    "textLength": 1001,
    "training": "trained",
    "gpu": 24,
    "glue": 86.4
  },
  {
    "name": "fastText",
    "github": "https://github.com/facebookresearch/fastText",
    "description": "Entwickelt wurde fastText in den \"Facebook AI Reasearch\" (FAIR) Laboren. Jedes Wort welches fastText verarbeitet, wird in ein \"n-gram\" von Wörtern zerlegt, um daraus eine Vektorrepräsentation zu erzeugen.",
    "download": "https://fasttext.cc/docs/en/english-vectors.html",
    "paper": "https://arxiv.org/abs/1607.04606",
    "languages": [
      {"multi": ""}
    ],
    "textLength": 1001,
    "training": "untrained",
    "gpu": 8,
    "glue": 0
  },
  {
    "name": "fastText-trained",
    "github": "https://github.com/facebookresearch/fastText",
    "description": "Entwickelt wurde fastText in den \"Facebook AI Reasearch\" (FAIR) Laboren. Jedes Wort welches fastText verarbeitet, wird in ein \"n-gram\" von Wörtern zerlegt, um daraus eine Vektorrepräsentation zu erzeugen.",
    "download": "https://fasttext.cc/docs/en/english-vectors.html",
    "paper": "https://arxiv.org/abs/1607.04606",
    "languages": [
      {"multi": "https://fasttext.cc/docs/en/crawl-vectors.html"},
      {"english": "https://fasttext.cc/docs/en/english-vectors.html"}
    ],
    "textLength": 1001,
    "training": "trained",
    "gpu": 8,
    "glue": 0
  }
]